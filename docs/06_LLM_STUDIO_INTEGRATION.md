# üè† INTEGRACI√ìN CON LLM STUDIO

> **Gu√≠a completa para conectar LLM Studio local con UTalk Backend**  
> **Versi√≥n**: 1.0.0  
> **Fecha**: 2025-08-20

## üìã Descripci√≥n

Esta gu√≠a explica c√≥mo integrar **LLM Studio** (ejecut√°ndose en una PC local) con el backend de UTalk que corre en Railway. LLM Studio permite ejecutar modelos de lenguaje localmente, reduciendo costos y mejorando la privacidad.

## üéØ Beneficios de LLM Studio

- **üí∞ Costo Cero**: No hay costos por tokens
- **üîí Privacidad Total**: Los datos nunca salen de tu red local
- **‚ö° Baja Latencia**: Respuestas m√°s r√°pidas para usuarios locales
- **üéõÔ∏è Control Total**: Configuraci√≥n completa de modelos
- **üìä Sin L√≠mites**: Sin restricciones de rate limiting externo

## üèóÔ∏è Arquitectura de Integraci√≥n

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    HTTP/API    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   UTalk Backend ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   LLM Studio    ‚îÇ
‚îÇ   (Railway)     ‚îÇ                ‚îÇ   (PC Local)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                   ‚îÇ
         ‚îÇ                                   ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Firebase‚îÇ                        ‚îÇ Modelos ‚îÇ
    ‚îÇ   DB    ‚îÇ                        ‚îÇ Locales ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîß Configuraci√≥n de LLM Studio

### 1. Instalaci√≥n y Configuraci√≥n

```bash
# 1. Descargar e instalar LLM Studio
# Visita: https://lmstudio.ai/

# 2. Abrir LLM Studio y configurar API
# Settings > API > Enable API Server
# Puerto: 3001 (o el que prefieras)
# Habilitar CORS si es necesario

# 3. Descargar modelos
# - gpt-oss-20b (recomendado)
# - llama-3.1-8b
# - mistral-7b
# - codellama-7b
```

### 2. Configuraci√≥n de Red

```bash
# Obtener IP de tu PC local
ifconfig  # macOS/Linux
ipconfig  # Windows

# Ejemplo: 192.168.1.100
# URL completa: http://192.168.1.100:3001
```

### 3. Verificar Funcionamiento

```bash
# Probar conexi√≥n b√°sica
curl http://192.168.1.100:3001

# Probar generaci√≥n de texto
curl -X POST http://192.168.1.100:3001/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-oss-20b",
    "prompt": "Hola, ¬øc√≥mo est√°s?",
    "max_tokens": 50
  }'
```

## ‚öôÔ∏è Configuraci√≥n en UTalk Backend

### 1. Variables de Entorno

```bash
# En Railway o .env
LLM_STUDIO_ENABLED=true
LLM_STUDIO_URL=http://192.168.1.100:3001
```

### 2. Configuraci√≥n por Workspace

```javascript
// Ejemplo de configuraci√≥n IA para workspace
{
  "ai_enabled": true,
  "provider": "llm_studio",
  "defaultModel": "gpt-oss-20b",
  "temperature": 0.3,
  "maxTokens": 150,
  "flags": {
    "suggestions": true,
    "rag": false,
    "provider_ready": true
  }
}
```

### 3. Probar Conexi√≥n

```bash
# Ejecutar script de prueba
node scripts/test-llm-studio-connection.js
```

## üîÑ Flujo de Integraci√≥n

### 1. Generaci√≥n de Sugerencias

```mermaid
sequenceDiagram
    participant U as Usuario
    participant B as Backend
    participant L as LLM Studio
    
    U->>B: Mensaje entrante
    B->>B: Cargar contexto
    B->>L: POST /v1/completions
    L->>L: Procesar con modelo local
    L->>B: Respuesta generada
    B->>B: Guardar sugerencia
    B->>U: Sugerencia disponible
```

### 2. Configuraci√≥n Autom√°tica

```javascript
// El sistema autom√°ticamente:
// 1. Detecta si LLM Studio est√° disponible
// 2. Usa LLM Studio como proveedor principal
// 3. Fallback a OpenAI si LLM Studio falla
// 4. Aplica rate limiting y circuit breaker
```

## üìä Monitoreo y M√©tricas

### 1. Health Checks

```bash
# Verificar estado de LLM Studio
GET /api/ai/health

# Respuesta esperada:
{
  "llm_studio": {
    "status": "healthy",
    "version": "0.2.0",
    "models": ["gpt-oss-20b", "llama-3.1-8b"]
  }
}
```

### 2. M√©tricas de Uso

```javascript
// M√©tricas disponibles:
{
  "provider": "llm_studio",
  "circuitBreaker": {
    "isOpen": false,
    "successRate": "95.2",
    "totalRequests": 150
  },
  "config": {
    "baseURL": "http://192.168.1.100:3001",
    "timeout": 10000,
    "rateLimitPerMinute": 10
  }
}
```

## üö® Troubleshooting

### Problemas Comunes

#### 1. Error de Conexi√≥n
```bash
# Error: ECONNREFUSED
# Soluci√≥n: Verificar que LLM Studio est√© ejecut√°ndose
```

#### 2. Timeout en Respuestas
```bash
# Error: ECONNABORTED
# Soluci√≥n: Aumentar timeout en configuraci√≥n
# Los modelos locales pueden tardar 10-30 segundos
```

#### 3. Modelo No Encontrado
```bash
# Error: Model not found
# Soluci√≥n: Verificar que el modelo est√© descargado en LLM Studio
```

### Logs de Debug

```bash
# Habilitar logs detallados
DEBUG=llm_studio:* npm start

# Ver logs en tiempo real
tail -f logs/app.log | grep "LLM Studio"
```

## üîí Seguridad

### 1. Configuraci√≥n de Red

```bash
# Recomendaciones de seguridad:
# - Usar red local privada
# - Configurar firewall si es necesario
# - No exponer LLM Studio a internet
```

### 2. Autenticaci√≥n

```javascript
// LLM Studio no requiere API key por defecto
// Para mayor seguridad, configurar autenticaci√≥n b√°sica:
{
  "auth": {
    "type": "basic",
    "username": "admin",
    "password": "secure_password"
  }
}
```

## üìà Optimizaci√≥n

### 1. Configuraci√≥n de Modelos

```javascript
// Configuraci√≥n optimizada para diferentes casos:
{
  "suggestions": {
    "model": "gpt-oss-20b",
    "temperature": 0.3,
    "maxTokens": 150
  },
  "reports": {
    "model": "llama-3.1-8b",
    "temperature": 0.1,
    "maxTokens": 500
  }
}
```

### 2. Rate Limiting

```javascript
// Configuraci√≥n de rate limiting:
{
  "rateLimitPerMinute": 10,  // Para modelos locales
  "burst": 5,                // Requests en burst
  "timeout": 30000           // 30 segundos para modelos locales
}
```

## üîÑ Migraci√≥n desde OpenAI

### 1. Configuraci√≥n Gradual

```javascript
// Estrategia de migraci√≥n:
// 1. Habilitar LLM Studio como fallback
// 2. Monitorear rendimiento
// 3. Cambiar a LLM Studio como principal
// 4. Mantener OpenAI como backup
```

### 2. Comparaci√≥n de Costos

```bash
# OpenAI GPT-4o-mini:
# - Entrada: $0.15 por 1M tokens
# - Salida: $0.60 por 1M tokens

# LLM Studio:
# - Costo: $0 (solo electricidad)
# - Hardware: GPU recomendada
```

## üìö Recursos Adicionales

### 1. Documentaci√≥n Oficial
- [LLM Studio Documentation](https://lmstudio.ai/docs)
- [API Reference](https://lmstudio.ai/docs/api)

### 2. Modelos Recomendados
- **gpt-oss-20b**: Mejor calidad general
- **llama-3.1-8b**: Buen balance calidad/velocidad
- **mistral-7b**: Excelente para espa√±ol
- **codellama-7b**: Especializado en c√≥digo

### 3. Hardware Recomendado
- **GPU**: NVIDIA RTX 3060 o superior
- **RAM**: 16GB m√≠nimo, 32GB recomendado
- **Storage**: SSD para mejor rendimiento

## üéØ Pr√≥ximos Pasos

1. **Configurar LLM Studio** en tu PC local
2. **Probar conexi√≥n** con el script de prueba
3. **Configurar variables de entorno** en Railway
4. **Habilitar LLM Studio** en un workspace de prueba
5. **Monitorear rendimiento** y ajustar configuraci√≥n
6. **Migrar gradualmente** desde OpenAI

---

**¬øNecesitas ayuda?** Revisa los logs del sistema o contacta al equipo de desarrollo. 