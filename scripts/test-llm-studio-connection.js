#!/usr/bin/env node

/**
 * ðŸ§ª SCRIPT DE PRUEBA PARA LLM STUDIO
 * 
 * Prueba la conexiÃ³n y funcionalidad de LLM Studio
 * 
 * @version 1.0.0
 * @author Backend Team
 */

require('dotenv').config();
const axios = require('axios');

// ConfiguraciÃ³n
const LLM_STUDIO_URL = process.env.LLM_STUDIO_URL || 'http://localhost:3001';
const TEST_PROMPT = "Eres un asistente Ãºtil. Responde brevemente: Â¿CuÃ¡l es la capital de EspaÃ±a?";
const TEST_MODEL = 'gpt-oss-20b';

/**
 * FunciÃ³n principal de prueba
 */
async function testLLMStudioConnection() {
  console.log('ðŸ§ª Iniciando pruebas de LLM Studio...\n');
  
  try {
    // 1. Probar conexiÃ³n bÃ¡sica
    console.log('1ï¸âƒ£ Probando conexiÃ³n bÃ¡sica...');
    await testBasicConnection();
    
    // 2. Probar endpoint de salud
    console.log('\n2ï¸âƒ£ Probando endpoint de salud...');
    await testHealthEndpoint();
    
    // 3. Probar generaciÃ³n de texto
    console.log('\n3ï¸âƒ£ Probando generaciÃ³n de texto...');
    await testTextGeneration();
    
    // 4. Probar modelos disponibles
    console.log('\n4ï¸âƒ£ Probando modelos disponibles...');
    await testAvailableModels();
    
    console.log('\nâœ… Todas las pruebas completadas exitosamente!');
    
  } catch (error) {
    console.error('\nâŒ Error en las pruebas:', error.message);
    process.exit(1);
  }
}

/**
 * Probar conexiÃ³n bÃ¡sica
 */
async function testBasicConnection() {
  try {
    const response = await axios.get(LLM_STUDIO_URL, {
      timeout: 5000,
      validateStatus: () => true // Aceptar cualquier status code
    });
    
    console.log(`   âœ… ConexiÃ³n exitosa a ${LLM_STUDIO_URL}`);
    console.log(`   ðŸ“Š Status Code: ${response.status}`);
    
    if (response.status === 200) {
      console.log('   ðŸŽ‰ LLM Studio estÃ¡ respondiendo correctamente');
    } else {
      console.log('   âš ï¸ LLM Studio responde pero con status inesperado');
    }
    
  } catch (error) {
    if (error.code === 'ECONNREFUSED') {
      throw new Error(`No se puede conectar a ${LLM_STUDIO_URL}. Verifica que LLM Studio estÃ© ejecutÃ¡ndose.`);
    } else if (error.code === 'ENOTFOUND') {
      throw new Error(`No se puede resolver la direcciÃ³n ${LLM_STUDIO_URL}. Verifica la URL.`);
    } else {
      throw new Error(`Error de conexiÃ³n: ${error.message}`);
    }
  }
}

/**
 * Probar endpoint de salud
 */
async function testHealthEndpoint() {
  try {
    const response = await axios.get(`${LLM_STUDIO_URL}/health`, {
      timeout: 5000
    });
    
    console.log('   âœ… Endpoint de salud responde correctamente');
    console.log(`   ðŸ“Š Status: ${response.status}`);
    
    if (response.data) {
      console.log('   ðŸ“‹ Datos de salud:', JSON.stringify(response.data, null, 2));
    }
    
  } catch (error) {
    if (error.response?.status === 404) {
      console.log('   âš ï¸ Endpoint /health no encontrado (puede ser normal)');
    } else {
      console.log(`   âš ï¸ Error en endpoint de salud: ${error.message}`);
    }
  }
}

/**
 * Probar generaciÃ³n de texto
 */
async function testTextGeneration() {
  try {
    const payload = {
      model: TEST_MODEL,
      prompt: TEST_PROMPT,
      temperature: 0.3,
      max_tokens: 100,
      stop: ['\n\n', 'Human:', 'Assistant:'],
      stream: false
    };
    
    console.log(`   ðŸ“ Enviando prompt: "${TEST_PROMPT}"`);
    console.log(`   ðŸ¤– Modelo: ${TEST_MODEL}`);
    
    const response = await axios.post(`${LLM_STUDIO_URL}/v1/completions`, payload, {
      timeout: 30000, // 30 segundos para modelos locales
      headers: {
        'Content-Type': 'application/json'
      }
    });
    
    console.log('   âœ… GeneraciÃ³n de texto exitosa');
    console.log(`   ðŸ“Š Status: ${response.status}`);
    
    const result = response.data;
    if (result.choices && result.choices[0]) {
      const generatedText = result.choices[0].text;
      console.log(`   ðŸ’¬ Respuesta: "${generatedText.trim()}"`);
      
      if (result.usage) {
        console.log(`   ðŸ“ˆ Tokens usados: ${result.usage.total_tokens || 'N/A'}`);
      }
    } else {
      console.log('   âš ï¸ Respuesta sin contenido generado');
    }
    
  } catch (error) {
    if (error.response?.status === 404) {
      throw new Error('Endpoint /v1/completions no encontrado. Verifica que LLM Studio tenga habilitada la API.');
    } else if (error.response?.status === 422) {
      console.log('   âš ï¸ Error de validaciÃ³n en el payload');
      if (error.response.data) {
        console.log('   ðŸ“‹ Detalles:', JSON.stringify(error.response.data, null, 2));
      }
    } else if (error.code === 'ECONNABORTED') {
      throw new Error('Timeout en la generaciÃ³n. Los modelos locales pueden tardar mÃ¡s tiempo.');
    } else {
      throw new Error(`Error en generaciÃ³n: ${error.message}`);
    }
  }
}

/**
 * Probar modelos disponibles
 */
async function testAvailableModels() {
  try {
    const response = await axios.get(`${LLM_STUDIO_URL}/v1/models`, {
      timeout: 5000
    });
    
    console.log('   âœ… Endpoint de modelos responde correctamente');
    
    if (response.data && response.data.data) {
      const models = response.data.data;
      console.log(`   ðŸ“‹ Modelos disponibles (${models.length}):`);
      
      models.forEach((model, index) => {
        console.log(`      ${index + 1}. ${model.id} (${model.object})`);
      });
    } else {
      console.log('   âš ï¸ No se encontraron modelos en la respuesta');
    }
    
  } catch (error) {
    if (error.response?.status === 404) {
      console.log('   âš ï¸ Endpoint /v1/models no encontrado (puede ser normal)');
    } else {
      console.log(`   âš ï¸ Error obteniendo modelos: ${error.message}`);
    }
  }
}

/**
 * Mostrar informaciÃ³n de configuraciÃ³n
 */
function showConfiguration() {
  console.log('ðŸ”§ ConfiguraciÃ³n actual:');
  console.log(`   URL: ${LLM_STUDIO_URL}`);
  console.log(`   Habilitado: ${process.env.LLM_STUDIO_ENABLED === 'true' ? 'SÃ­' : 'No'}`);
  console.log(`   Modelo de prueba: ${TEST_MODEL}`);
  console.log('');
}

// Ejecutar pruebas
if (require.main === module) {
  showConfiguration();
  testLLMStudioConnection()
    .then(() => {
      console.log('\nðŸŽ‰ Pruebas completadas!');
      process.exit(0);
    })
    .catch((error) => {
      console.error('\nðŸ’¥ Error en las pruebas:', error.message);
      process.exit(1);
    });
}

module.exports = {
  testLLMStudioConnection,
  testBasicConnection,
  testHealthEndpoint,
  testTextGeneration,
  testAvailableModels
}; 